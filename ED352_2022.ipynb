{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML IDPASC2021.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanncoadou/MLtutorials/blob/main/ED352_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcL2rlpraCL2"
      },
      "source": [
        "<h1>IDPASC 2021 Machine learning hands-on</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYV6fbcnXIR2"
      },
      "source": [
        "# Standard imports and practical functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HnV_PMdpCI7"
      },
      "source": [
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification, make_circles\n",
        "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns # seaborn for nice plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(31415) # set the np random seed for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATAV2LehCsHL"
      },
      "source": [
        "### Function to plot decision contours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBSIEt7OX2R8"
      },
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "\n",
        "def my_plot_decision_regions(model, X, y, alpha=1.0, size=25, npts=10000, zoom=0.25, event5=False):\n",
        "  x1min = X[:,0].min() - zoom\n",
        "  x1max = X[:,0].max() + zoom\n",
        "\n",
        "  x2min = X[:,1].min() - zoom\n",
        "  x2max = X[:,1].max() + zoom\n",
        "  \n",
        "  x1 = np.random.uniform(x1min, x1max, npts)\n",
        "  x2 = np.random.uniform(x2min, x2max, npts)\n",
        "\n",
        "  if hasattr(model, \"predict_proba\"):\n",
        "    z = model.predict_proba(np.vstack((x1,x2)).T)\n",
        "  else:\n",
        "    z = model.predict(np.vstack((x1,x2)).T)\n",
        "  \n",
        "  if len(z.shape) == 2:\n",
        "    if z.shape[1] == 1:\n",
        "      z = z.reshape(-1)\n",
        "    elif z.shape[1] == 2:\n",
        "      z = z[:,1].reshape(-1)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  bottom = cm.get_cmap('Oranges', 128)\n",
        "  top = cm.get_cmap('Blues_r', 128)\n",
        "\n",
        "  newcolors = np.vstack((top(np.linspace(0, 1, 128+128)[-128:]),\n",
        "                        bottom(np.linspace(0, 1, 128+128)[:128])))\n",
        "  newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
        "\n",
        "\n",
        "  ax.tricontour(x1, x2, z, levels=np.linspace(0.0-np.finfo(float).eps,1.0+np.finfo(float).eps,20,True), linewidths=0.1, colors='k', antialiased=True)\n",
        "  cntr = ax.tricontourf(x1, x2, z, levels=np.linspace(0.0-np.finfo(float).eps,1.0+np.finfo(float).eps,20,True), cmap=newcmp)\n",
        "  sctr0 = ax.scatter(X[y==0][:,0], X[y==0][:,1], alpha=alpha, s=size, c=\"#1f77b4\", marker=\"s\", edgecolors=\"k\", linewidths=0.5)\n",
        "  sctr1 = ax.scatter(X[y==1][:,0], X[y==1][:,1], alpha=alpha, s=size, c=\"#ff7f0e\",  marker=\"^\", edgecolors=\"k\", linewidths=0.5)\n",
        "  if event5: # showing particular swinger event\n",
        "    sctr2 = ax.scatter(X[4][0], X[4][1], alpha=1, s=size*10, c=\"lightgreen\",  marker=\"X\", edgecolors=\"k\", linewidths=1)\n",
        "  fig.colorbar(cntr, ax=ax)\n",
        "  # ax.set(xlim=(x1min, x1max), ylim=(x2min, x2max))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQWreSYyC63_"
      },
      "source": [
        "### Function to plot ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjBgEqYSf9ke"
      },
      "source": [
        "def my_plot_roc_curve(model, X_test, y_test):\n",
        "  if hasattr(model, \"predict_proba\"):\n",
        "    y_scores = model.predict_proba(X_test)\n",
        "  else:\n",
        "    y_scores = model.predict(X_test)\n",
        "\n",
        "  if len(y_scores.shape) == 2:\n",
        "    if y_scores.shape[1] == 1:\n",
        "      y_scores = y_scores.reshape(-1)\n",
        "    elif y_scores.shape[1] == 2:\n",
        "      y_scores = y_scores[:,1].reshape(-1)\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "  roc_auc = roc_auc_score(y_test, y_scores)\n",
        "  plt.clf()\n",
        "  display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=model.__class__.__name__)\n",
        "  display.plot()\n",
        "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkZHLdFdEDSO"
      },
      "source": [
        "# Classifier zoo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTL8w9wsxHHJ"
      },
      "source": [
        "# X = (x,y) coordinates; y = class\n",
        "X1, y1 = make_circles(n_samples=1000, noise=0.1, factor=0.8)\n",
        "X2, y2 = make_circles(n_samples=1000, noise=0.2, factor=0.2)\n",
        "X = np.vstack((X1,X2/2))\n",
        "y = np.hstack((y1,y2))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xKG-csE30o1"
      },
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENciMbPgs1of"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZVxFAovs-aB"
      },
      "source": [
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm6--R3vIsHx"
      },
      "source": [
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(15,10))\n",
        "plot_tree(dtc)\n",
        "plt.show();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWtnH5mfHoAc"
      },
      "source": [
        "accuracy_score(y_test, dtc.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws6PLQDMXomi"
      },
      "source": [
        "Access to results:\n",
        "- `predict` returns the class (0 or 1 if binary classifier)\n",
        "- `predict_proba` returns the probability of each class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMELaRV8WykV"
      },
      "source": [
        "print(\"predict: \\n\",dtc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",dtc.predict_proba(X_test[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ebgN5Ciu_MU"
      },
      "source": [
        "try:\n",
        "  from mlxtend.plotting import plot_decision_regions\n",
        "except ImportError as e:\n",
        "  !pip install mlxtend\n",
        "  from mlxtend.plotting import plot_decision_regions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xQIXsMmvHqT"
      },
      "source": [
        "# practical but limited contour-plotting function\n",
        "plot_decision_regions(X_test, y_test, dtc);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_iWuFOEYTL5"
      },
      "source": [
        "# defined at top of notebook\n",
        "# can use class (0 or 1) or class probability when available\n",
        "my_plot_decision_regions(dtc, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWUVu3GmISTi"
      },
      "source": [
        "my_plot_roc_curve(dtc, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7GDJuCPuzp"
      },
      "source": [
        "### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBsjZC-1Puzt"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjkcRs55Puzx"
      },
      "source": [
        "#abc = AdaBoostClassifier()\n",
        "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),n_estimators=100)\n",
        "abc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ANOY5aZYMK7"
      },
      "source": [
        "print(\"predict: \\n\",abc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",abc.predict_proba(X_test[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El7EfSiKPuz6"
      },
      "source": [
        "my_plot_decision_regions(abc, X_test, y_test)\n",
        "my_plot_roc_curve(abc, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQOs0B2ahQZc"
      },
      "source": [
        "### Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA3nD4rov9F5"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5RhTx8fv_jj"
      },
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators=400)\n",
        "gbc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_92fwlWPYxyk"
      },
      "source": [
        "print(\"predict: \\n\",gbc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",gbc.predict_proba(X_test[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e27dSIbwwkUT"
      },
      "source": [
        "my_plot_decision_regions(gbc, X_test, y_test, event5=True)\n",
        "my_plot_roc_curve(gbc, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8k241SKLPiV"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaIRJLaTLPiZ"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvj_qdQILPib"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=400)\n",
        "rfc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtq-96fBe4c7"
      },
      "source": [
        "print(\"predict: \\n\",rfc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",rfc.predict_proba(X_test[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9WlAQvDLPie"
      },
      "source": [
        "my_plot_decision_regions(rfc, X_test, y_test)\n",
        "my_plot_roc_curve(rfc, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p6edHiVc4Id"
      },
      "source": [
        "### Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGSnlX_yLPif"
      },
      "source": [
        "y_preds_dtc = dtc.predict_proba(X_test)[:,1].reshape(-1)\n",
        "y_preds_abc = abc.predict_proba(X_test)[:,1].reshape(-1)\n",
        "y_preds_gbc = gbc.predict_proba(X_test)[:,1].reshape(-1)\n",
        "y_preds_rfc = rfc.predict_proba(X_test)[:,1].reshape(-1)\n",
        "fpr_dtc,tpr_dtc,_ = roc_curve(y_true=y_test, y_score=y_preds_dtc)\n",
        "fpr_abc,tpr_abc,_ = roc_curve(y_true=y_test, y_score=y_preds_abc)\n",
        "fpr_gbc,tpr_gbc,_ = roc_curve(y_true=y_test, y_score=y_preds_gbc)\n",
        "fpr_rfc,tpr_rfc,_ = roc_curve(y_true=y_test, y_score=y_preds_rfc)\n",
        "auc_test_dtc = roc_auc_score(y_true=y_test, y_score=y_preds_dtc)\n",
        "auc_test_abc = roc_auc_score(y_true=y_test, y_score=y_preds_abc)\n",
        "auc_test_gbc = roc_auc_score(y_true=y_test, y_score=y_preds_gbc)\n",
        "auc_test_rfc = roc_auc_score(y_true=y_test, y_score=y_preds_rfc)\n",
        "plt.plot(fpr_dtc, tpr_dtc, color='darkblue',label='{} (AUC  = {})'.format(dtc.__class__.__name__,np.round(auc_test_dtc,decimals=2)))\n",
        "plt.plot(fpr_abc, tpr_abc, color='darkred',label='{} (AUC  = {})'.format(abc.__class__.__name__,np.round(auc_test_abc,decimals=2)))\n",
        "plt.plot(fpr_gbc, tpr_gbc, color='darkgreen',label='{} (AUC  = {})'.format(gbc.__class__.__name__,np.round(auc_test_gbc,decimals=2)))\n",
        "plt.plot(fpr_rfc, tpr_rfc, color='darkorange',label='{} (AUC  = {})'.format(rfc.__class__.__name__,np.round(auc_test_rfc,decimals=2)))\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW_XQQitNYr6"
      },
      "source": [
        "### Neural networks\n",
        "It is possible to use neural networks from scikit-learn (`from sklearn.neural_network import MLPClassifier`). In the following we will use more advanced implementations with [Tensorflow](https://www.tensorflow.org)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH2EWtTyO9LW"
      },
      "source": [
        "try:\n",
        "  import tensorflow as tf\n",
        "except ImportError as e:\n",
        "  !pip install tensorflow\n",
        "  import tensorflow as tf\n",
        "print (tf.__version__)  # preinstalled version 2.6.0 20210824\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3q5ZVvoUhJn"
      },
      "source": [
        "model = keras.models.Sequential(\n",
        "    [\n",
        "     keras.layers.Dense(128, activation='relu', input_shape=(2,)),\n",
        "     keras.layers.Dense(128, activation='relu'),\n",
        "     keras.layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy', keras.metrics.AUC()])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AswSNi8GVDI6"
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "          validation_split=0.2,\n",
        "          epochs=1000,\n",
        "          callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INKEpnuRw0fp"
      },
      "source": [
        "print(\"predict: \\n\",model.predict(X_test[:5]))\n",
        "accuracy_score(y_test, model.predict(X_test).round())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8E9MkZGNYsL"
      },
      "source": [
        "my_plot_decision_regions(model, X_test, y_test)\n",
        "my_plot_roc_curve(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu9qvFvthvhK"
      },
      "source": [
        "y_preds_model = model.predict(X_test)\n",
        "fpr_model,tpr_model,_ = roc_curve(y_true=y_test, y_score=y_preds_model)\n",
        "auc_test_model = roc_auc_score(y_true=y_test, y_score=y_preds_model)\n",
        "plt.plot(fpr_rfc, tpr_rfc, color='darkorange',label='{} (AUC  = {})'.format(rfc.__class__.__name__,np.round(auc_test_rfc,decimals=2)))\n",
        "plt.plot(fpr_model, tpr_model, color='purple',label='{} (AUC  = {})'.format(\"Neural network\",np.round(auc_test_model,decimals=2)))\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNk6hXogjTyP"
      },
      "source": [
        "# High energy physics application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0TmE1VLlxyG"
      },
      "source": [
        "## Input dataset\n",
        "\n",
        "Data created from ATLAS Open Data by David Rousseau. See doc:\n",
        "\n",
        "http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iA5CK6DmhPi"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROMhtcGHZLbc"
      },
      "source": [
        "import os\n",
        "filename=\"dataWW_d1_600k.csv.gz\"\n",
        "if not os.path.isfile(filename):\n",
        "  try:\n",
        "    import gdown\n",
        "  except ImportError as e:\n",
        "    !pip install gdown\n",
        "    import gdown\n",
        "  !gdown https://drive.google.com/uc?id=1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4\n",
        "!ls -lrt\n",
        "\n",
        "# Loading dataset\n",
        "dfall = pd.read_csv(filename) \n",
        "print (\"\\nFile loaded with \",dfall.shape[0], \" events \")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pum_u34Dlib_"
      },
      "source": [
        "After downloading message and directory listing, you should now see:\n",
        "\n",
        "`File loaded with  600000  events`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms_kAr_nmWL3"
      },
      "source": [
        "### Checking the content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtKOBCbam1pD"
      },
      "source": [
        "#dumping list of features\n",
        "dfall.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC3MydYcnBzE"
      },
      "source": [
        "#examining first few events\n",
        "display(dfall.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb-Pv4TXnNPQ"
      },
      "source": [
        "#examining feature distributions\n",
        "dfall.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Z7EYWMnyxk"
      },
      "source": [
        "***Event weights***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0VbTZ3dnsNB"
      },
      "source": [
        "label_nevents = (dfall[dfall.label==0].shape[0], dfall[dfall.label==1].shape[0] )\n",
        "print(\"Number of events per class (B, S):\",label_nevents)\n",
        "\n",
        "label_weights = (dfall[dfall.label==0].mcWeight.sum(), dfall[dfall.label==1].mcWeight.sum() ) \n",
        "print(\"Total weight per class (B, S):    \",label_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38t5fWDRf967"
      },
      "source": [
        "## Event selection\n",
        "\n",
        "Only keep events with exactly two leptons for this exercise.\n",
        "\n",
        "Only keep events with positive weight, as many ML tools choke on negative weight.\n",
        "\n",
        "*Note: This is in principle WRONG, only valid if your positive and negative weight events are statistically similar (could then also take the absolute value of the weight to increase statistics).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2wvNoDqgDPq"
      },
      "source": [
        "print (\"Df shape before selection:\", dfall.shape)\n",
        "\n",
        "fulldata=dfall[ (dfall.lep_n==2) & (dfall.mcWeight > 0)]  \n",
        "\n",
        "print (\"Df shape after selection: \",fulldata.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM01cgNliKeV"
      },
      "source": [
        "# Hide label and weights in separate vectors (not discriminating features)\n",
        "# WARNING : there should be neither selection nor shuffling later on! (otherwise misalignement)\n",
        "target = fulldata[\"label\"]\n",
        "weights = fulldata[\"mcWeight\"]\n",
        "\n",
        "# for simplicity only keep some features\n",
        "# this is actually making a deep copy from fulldata\n",
        "data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1'])\n",
        "#data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
        "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']\n",
        "\n",
        "print (\"Df shape of dataset to be used:\",data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKB2Zk3fvPMV"
      },
      "source": [
        "### Event weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXRIxOApuus7"
      },
      "source": [
        "fig,ax=plt.subplots()\n",
        "\n",
        "bins=np.linspace(-1,1,101)\n",
        "plt.hist(weights[target==0]*1000,bins=bins,color='b',alpha=0.5,density=True,label='B ackground')\n",
        "plt.hist(weights[target==1]*10000,bins=bins,color='r',alpha=0.5,density=True,label='S ignal (Wx10)')\n",
        "plt.legend(loc='best')\n",
        "ax.set_xlabel('weight*1000')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5o3BdN0vWRS"
      },
      "source": [
        "### Feature engineering\n",
        "\n",
        "Add more complex variables to the dataset.\n",
        "\n",
        "*Do this later if time permits.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgeyQKNgvkOx"
      },
      "source": [
        "if False: \n",
        "    data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
        "\n",
        "    print (data.shape)\n",
        "    display(data.head())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEzVz2fUvUNs"
      },
      "source": [
        "### Plotting variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16cH0csMjoLZ"
      },
      "source": [
        "fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "data['met_et'].plot.hist(title='Missing Transverse Energy', log=True, ax=ax[0])\n",
        "data[data.lep_pt_0+data.lep_pt_1>1000]['met_et'].plot.hist(bins=np.linspace(0,400,50),title='Missing Transverse Energy for large lepton Pt', ax=ax[1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqwSM579lxQI"
      },
      "source": [
        "ax=data[target==0].plot.scatter(x='met_et', y='lep_pt_0',color=\"b\",label=\"B\")\n",
        "data[target==1].plot.scatter(x='met_et', y='lep_pt_0',color=\"r\",label=\"S\",alpha=.5,ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEKfvFJcuMdt"
      },
      "source": [
        "data[data.lep_pt_0+data.lep_pt_1>2000].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lFsOPQfvpK7"
      },
      "source": [
        "ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),bins=50,color='b',alpha=0.5,density=True,label=\"B\")\n",
        "ax=ax.flatten()[:data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
        "data[target==1].hist(weights=weights[target==1],figsize=(15,12),bins=50,color='r',alpha=0.5,density=True,ax=ax,label=\"S\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3xGtyYVoRm"
      },
      "source": [
        "### Features correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DYCWyOalVoRn"
      },
      "source": [
        "fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "corrMatrix = data[target==0].corr()\n",
        "ax[0].set_title(\"Background features correlation matrix\")\n",
        "sns.heatmap(corrMatrix.round(3), ax=ax[0], annot=True);\n",
        "\n",
        "corrMatrix = data[target==1].corr()\n",
        "ax[1].set_title(\"Signal features correlation matrix\")\n",
        "sns.heatmap(corrMatrix.round(3), ax=ax[1], annot=True);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iekW3mH_2bv4"
      },
      "source": [
        "## Sample splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd_EetqH2XRo"
      },
      "source": [
        "np.random.seed(31415) # set the random seed (used for the train/test splitting)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_size = 0.75 # fraction of sample used for training\n",
        "val_size = 0.2 # fraction of training sample used for validation\n",
        "\n",
        "# split only train/test\n",
        "#X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "#    train_test_split(data, target, weights, train_size=train_size)\n",
        "\n",
        "#split in train/validation/test\n",
        "X_holdout, X_test, y_holdout, y_test, weights_holdout, weights_test = \\\n",
        "    train_test_split(data, target, weights, train_size=train_size)\n",
        "X_train, X_val, y_train, y_val, weights_train, weights_val = \\\n",
        "    train_test_split(X_holdout, y_holdout, weights_holdout, train_size=1-val_size)\n",
        "\n",
        "print(\"Training sample:  \", X_train.shape)\n",
        "print(\"Validation sample:\", X_val.shape)\n",
        "print(\"Testing sample:   \", X_test.shape)\n",
        "\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "print (\"class_weights_train (B, S):\",class_weights_train)\n",
        "\n",
        "for i in range(len(class_weights_train)):\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
        "    weights_test[y_test == i] *= 1/(1-train_size) # increase test weight to compensate for sampling\n",
        "    weights_val[y_val == i] *= 1/val_size/train_size # increase val weight to compensate for samplings\n",
        "    \n",
        "print (\"Test:  total weight sig\", weights_test[y_test == 1].sum())\n",
        "print (\"Test:  total weight bkg\", weights_test[y_test == 0].sum())\n",
        "print (\"Train: total weight sig\", weights_train[y_train == 1].sum())\n",
        "print (\"Train: total weight bkg\", weights_train[y_train == 0].sum())\n",
        "print (\"Val:   total weight sig\", weights_val[y_val == 1].sum())\n",
        "print (\"Val:   total weight bkg\", weights_val[y_val == 0].sum())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyEFjed1EJmc"
      },
      "source": [
        "## Network training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lvkMyyLUNTT"
      },
      "source": [
        "try:\n",
        "  import tensorflow as tf\n",
        "except ImportError as e:\n",
        "  !pip install tensorflow\n",
        "  import tensorflow as tf\n",
        "print (tf.__version__)  # preinstalled version 2.6.0 20210824\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwfo4e_J_qEJ"
      },
      "source": [
        "tf.random.set_seed(1234) # to have reproducible networks\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)), # 1st hidden layer\n",
        "  #tf.keras.layers.Dense(128, activation='relu'), # 2nd hidden layer\n",
        "  tf.keras.layers.Dense(1,activation=\"sigmoid\") # output layer\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              #metrics=['accuracy', keras.metrics.AUC(name=\"auc\")]) # if not using event weights\n",
        "              weighted_metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "history = model.fit(X_train, y_train.values,\n",
        "                    epochs=1,\n",
        "                    #validation_split=0.2,   # to be used with train/test split\n",
        "                    validation_data=(X_val, y_val, weights_val),\n",
        "                    batch_size=1024,\n",
        "                    sample_weight=weights_train.values,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMI4RQAhEVZP"
      },
      "source": [
        "### Standardisation of inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHW-U78IFdeS"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Original mean and variance:\")\n",
        "for feature, mean, std in zip(data.columns,X_train.mean(0), X_train.std(0)):\n",
        "  print(\"{:9}: {:7.4f} +/- {:7.4f}\".format(feature,mean,std))\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "# in training sample\n",
        "scaler = StandardScaler()\n",
        "# \".values[:]\" to keep dataframe and not convert to numpy array\n",
        "X_train.values[:] = scaler.fit_transform(X_train)\n",
        "# apply to testing/validation sample the transformation calculated on training sample\n",
        "X_test.values[:] = scaler.transform(X_test)\n",
        "X_val.values[:] = scaler.transform(X_val)\n",
        "\n",
        "print(\"\\nStandardised mean and variance:\")\n",
        "for feature, mean, std in zip(data.columns,X_train.mean(0), X_train.std(0)):\n",
        "  print(\"{:9}: {:7.4f} +/- {:7.4f}\".format(feature,mean,std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxVheGRKFvZ-"
      },
      "source": [
        "tf.random.set_seed(1234) # to have reproducible networks\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)), # 1st hidden layer\n",
        "  #tf.keras.layers.Dense(128, activation='relu'), # 2nd hidden layer\n",
        "  tf.keras.layers.Dense(1,activation=\"sigmoid\") # output layer\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              #metrics=['accuracy', keras.metrics.AUC(name=\"auc\")]) # if not using event weights\n",
        "              weighted_metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n",
        "\n",
        "history = model.fit(X_train, y_train.values,\n",
        "                    epochs=100,\n",
        "                    #validation_split=0.2,   # to be used with train/test split\n",
        "                    validation_data=(X_val, y_val, weights_val),\n",
        "                    batch_size=1024,\n",
        "                    sample_weight=weights_train.values,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
        "\n",
        "y_pred_model = model.predict(X_test).reshape(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f842cWRvMBnV"
      },
      "source": [
        "*Compare the training loss/accuracy/AUC after epoch 1 with that obtained before standardisation.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnjgYUWN5o-V"
      },
      "source": [
        "density=True   # normalised to 1 (=> probability density function)\n",
        "#density=False   # normalised to one year at LHC\n",
        "\n",
        "plt.hist(y_pred_model[y_test == 0],\n",
        "         color='b', alpha=0.5, \n",
        "         bins=30,\n",
        "         histtype='stepfilled', density=density,\n",
        "         label='B (test)', weights=weights_test[y_test == 0])\n",
        "plt.hist(y_pred_model[y_test == 1],\n",
        "         color='r', alpha=0.5,\n",
        "         bins=30,\n",
        "         histtype='stepfilled', density=density,\n",
        "         label='S (test)', weights=weights_test[y_test == 1])\n",
        "plt.legend()\n",
        "plt.title(\"NN model score\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8IqX99nRvwi"
      },
      "source": [
        "my_plot_roc_curve(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cTdFbz7gFW"
      },
      "source": [
        "### Training monitoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRTjASg9yXeW"
      },
      "source": [
        "fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax[0].plot(history.history['loss'],label=\"Training loss\")\n",
        "ax[0].plot(history.history['val_loss'],label=\"Validation loss\")\n",
        "ax[0].set_xlabel(\"Epoch\")\n",
        "ax[0].legend(loc='best');\n",
        "\n",
        "ax[1].plot(history.history['auc'],label=\"Training AUC\")\n",
        "ax[1].plot(history.history['val_auc'],label=\"Validation AUC\")\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].legend(loc='best');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvY8OfksdT08"
      },
      "source": [
        "### Model saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h9ktbq9cxQi"
      },
      "source": [
        "***Whole-model saving & loading***\n",
        "\n",
        "You can save an entire model to a directory. It will include:\n",
        "- the model's architecture/config\n",
        "- the model's weight values (which were learned during training)\n",
        "- the model's compilation information (if `compile()` was called)\n",
        "- the optimizer and its state, if any (this enables you to restart training where you left)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09els9DAcNP4"
      },
      "source": [
        "model.save(\"NNmodel\")\n",
        "!ls -a NNmodel/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIb7O2S3cbyY"
      },
      "source": [
        "print(\"Prediction from original model:\")\n",
        "display(model.predict(X_test[:5]))\n",
        "\n",
        "reloaded_model=keras.models.load_model(\"NNmodel\")\n",
        "print(\"Prediction from reloaded model:\")\n",
        "display(reloaded_model.predict(X_test[:5]))\n",
        "#np.testing.assert_allclose(\n",
        "#    model.predict(X_test), reloaded_model.predict(X_test)\n",
        "#)\n",
        "\n",
        "# further training\n",
        "reloaded_model.fit(X_train, y_train.values,\n",
        "                   epochs=5,\n",
        "                   #validation_split=0.2,   # to be used with train/test split\n",
        "                   validation_data=(X_val, y_val, weights_val),\n",
        "                   batch_size=1024,\n",
        "                   sample_weight=weights_train.values,\n",
        "                   callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0P98kVryCqf"
      },
      "source": [
        "*Compare first epoch values with original training.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zaRnkjfedo"
      },
      "source": [
        "***Partial save***\n",
        "\n",
        "Save a single HDF5 file containing the model's architecture, weights values, and `compile()` information.\n",
        "\n",
        "Not saved (to be provided separately to resume training):\n",
        "- external losses & metrics added via `model.add_loss()` & `model.add_metric()`\n",
        "- computation graph of custom objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReCf3qAkgG8r"
      },
      "source": [
        "model.save(\"NNmodel.h5\")\n",
        "!ls -lrt --color\n",
        "print(\"\\nPrediction from original model:\")\n",
        "display(model.predict(X_test[:5]))\n",
        "\n",
        "reloaded_model=keras.models.load_model(\"NNmodel.h5\")\n",
        "print(\"Prediction from reloaded model:\")\n",
        "display(reloaded_model.predict(X_test[:5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnUTf8IRjFQq"
      },
      "source": [
        "***Saving the architecture and weights***\n",
        "\n",
        "Keeping the model's configuration and training weights in separate files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_7dbAVQjPfF"
      },
      "source": [
        "arch = model.to_json()\n",
        "with open('NNmodel.json', 'w') as arch_file:\n",
        "  arch_file.write(arch)\n",
        "model.save_weights('NNmodel_weights.h5')\n",
        "!ls -lrt --color"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiXYkGY6kYJ6"
      },
      "source": [
        "!python -m json.tool NNmodel.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3xolQc-nB3E"
      },
      "source": [
        "with open('NNmodel.json', 'r') as f:\n",
        "  reloaded_model = keras.models.model_from_json(f.read())\n",
        "reloaded_model.summary()\n",
        "\n",
        "reloaded_model.load_weights(\"NNmodel_weights.h5\")\n",
        "#reloaded_model.compile(loss=\"binary_crossentropy\",\n",
        "#                       optimizer=\"adam\",\n",
        "#                       #metrics=['accuracy', keras.metrics.AUC(name=\"auc\")]) # if not using event weights\n",
        "#                       weighted_metrics=['accuracy', keras.metrics.AUC(name=\"auc\")])\n",
        "reloaded_model.fit(X_train, y_train.values,\n",
        "                    epochs=1,\n",
        "                    #validation_split=0.2,   # to be used with train/test split\n",
        "                    validation_data=(X_val, y_val, weights_val),\n",
        "                    batch_size=1024,\n",
        "                    sample_weight=weights_train.values,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWsfyeZzjS9"
      },
      "source": [
        "## Physics performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsMSGl-Kwql"
      },
      "source": [
        "### Significance\n",
        "\n",
        "Asimov significance (from [arXiv:1007.1727](https://arxiv.org/abs/1007.1727) eq. 97):\n",
        "\n",
        "> AMS = $\\sqrt{2\\left((s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s\\right)} = \\frac{s}{\\sqrt{b}}\\left(1+\\mathcal{O}(s/b)\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qubY3CMNKwql"
      },
      "source": [
        "from math import sqrt\n",
        "from math import log\n",
        "def amsasimov(s,b):\n",
        "  if b<=0 or s<=0:\n",
        "      return 0\n",
        "  try:\n",
        "      return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
        "  except ValueError:\n",
        "      print(1+float(s)/b)\n",
        "      print (2*((s+b)*log(1+float(s)/b)-s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxd3r5_Q2NRD"
      },
      "source": [
        "int_pred_test_sig_model = [weights_test[(y_test ==1) & (y_pred_model > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "int_pred_test_bkg_model = [weights_test[(y_test ==0) & (y_pred_model > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "vamsasimov_model = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_model,int_pred_test_bkg_model)]\n",
        "print(\"Z: \",np.round(max(vamsasimov_model),decimals=3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcX-9xla1-I6"
      },
      "source": [
        "plt.plot(np.linspace(0,1,num=50),vamsasimov_model, label='AMS (Z_max = {})'.format(np.round(max(vamsasimov_model),decimals=3)))\n",
        "\n",
        "plt.title(\"Significance\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Significance\")\n",
        "plt.legend()\n",
        "#plt.savefig(\"Significance.pdf\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YniuFUBNMeQ"
      },
      "source": [
        "### Feature importance\n",
        "Feature importance allows to display the importance of each feature without rerunnning the training. It is obtained from internal algorithm quantities, like cumulated decrease of impurity. Magnitude is arbitrary. It can be used as a not very reliable indication of which feature is the most discriminant.\n",
        "\n",
        "Very straightforward with decision trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQEL8hnNEcg"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O-fUIiJM7w4"
      },
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators=10,verbose=1)\n",
        "gbc.fit(X_train, y_train, sample_weight=weights_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHVWv_f5NMeR"
      },
      "source": [
        "plt.bar(data.columns.values, gbc.feature_importances_)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Feature importance\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXVHA_8g4gx"
      },
      "source": [
        "*What about a different tree classifier?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EMI8wZaQCT9"
      },
      "source": [
        "# preinstalled version 0.9.0 20210824\n",
        "!pip install xgboost --upgrade # install 1.4.2 20210824\n",
        "import xgboost as xgb\n",
        "print(xgb.__version__)\n",
        "\n",
        "useLGB=False #Could also use LightGBM\n",
        "# preinstalled version 2.2.3 20210824\n",
        "if useLGB:\n",
        "  !pip install lightgbm --upgrade # install 3.2.1 20210824\n",
        "  import lightgbm as lgb\n",
        "  print (lgb.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkTl8I8PyMi"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features (not a bad idea to double check without it)\n",
        "# can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
        "# use_label_encoder and eval_metric to silence warning in 1.3.0\n",
        "xgb = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss')\n",
        "\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values) # note that XGB 1.3.X requires positive weight\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z04B3jCjjSZj"
      },
      "source": [
        "if useLGB:\n",
        "  gbm = lgb.LGBMClassifier()\n",
        "  gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0gQm1QEQ4aY"
      },
      "source": [
        "if useLGB:\n",
        "  fig,ax=plt.subplots(1, 3, figsize=(18, 5))\n",
        "else:\n",
        "  fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax[0].bar(data.columns.values, xgb.feature_importances_)\n",
        "ax[0].tick_params(labelrotation=90)\n",
        "ax[0].set_title(\"XGBoost feature importance\")\n",
        "ax[1].bar(data.columns.values, gbc.feature_importances_)\n",
        "ax[1].tick_params(labelrotation=90)\n",
        "ax[1].set_title(\"sklearn feature importance\");\n",
        "if useLGB:\n",
        "  ax[2].bar(data.columns.values, gbm.feature_importances_)\n",
        "  ax[2].tick_params(labelrotation=90)\n",
        "  ax[2].set_title(\"LightGBM feature importance\");\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xPybEujNMeS"
      },
      "source": [
        "### Permutation importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiPAfpWquvqP"
      },
      "source": [
        "A better way to show the importance of each feature is Permutation Importance, where each feature in turn is replaced by an instance of an other event (effectively switching it off by randomising).\n",
        "\n",
        "Works on any classifier, not just DT-based.\n",
        "\n",
        "However, report can be misleading in case of highly correlated variables.\n",
        "\n",
        "Available in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) but without event weights in Colab version (0.22).\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZb-TCA879lg"
      },
      "source": [
        "if False:\n",
        "  from sklearn.inspection import permutation_importance\n",
        "  result_xgb = permutation_importance(xgb, X_test, y_test, n_repeats=1, random_state=42, n_jobs=2)\n",
        "  forest_importances_xgb = pd.Series(result_xgb.importances_mean, index=list(data.columns.values))\n",
        "\n",
        "  if useLGB:\n",
        "    result_gbm = permutation_importance(gbm, X_test, y_test, n_repeats=1, random_state=42, n_jobs=2)\n",
        "    forest_importances_gbm = pd.Series(result_gbm.importances_mean, index=list(data.columns.values))\n",
        "\n",
        "  if useLGB:\n",
        "    fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "    forest_importances_xgb.plot.bar(ax = ax[0], subplots=True)\n",
        "    ax[0].set_title(\"XGBoost permutation importance\")\n",
        "  else:\n",
        "    fig,ax=plt.subplots()\n",
        "    forest_importances_xgb.plot.bar()\n",
        "    ax.set_title(\"XGBoost permutation importance\")\n",
        "  if useLGB:\n",
        "    forest_importances_gbm.plot.bar(ax = ax[1], subplots=True)\n",
        "    ax[1].set_title(\"LightGBM permutation importance\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_7NJTDOF0ss"
      },
      "source": [
        "Another implementation targetting HEP:\n",
        "\n",
        "https://github.com/aghoshpub/permutationImportancePhysics \n",
        "\n",
        "In particular it allows to : \n",
        "   * use event weights\n",
        "   * display directly the loss in whatever criterion (ROC auc, asimov significance) when the feature is switched off\n",
        "   * display the feature importance for a specific subset (for example the most signal like)\n",
        "   * it can even display which feature has the largest impact on systematics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZsHARtTuvqQ"
      },
      "source": [
        "if False:\n",
        "    !pip install PermutationImportancePhysics\n",
        "    from permutationimportancephysics.PermutationImportance import PermulationImportance # note the delibrate typo PermuLation\n",
        "    #XGBoost\n",
        "    PI_xgb = PermulationImportance(model=xgb, X=X_test.values,y=y_test,weights=weights_test,\\\n",
        "                           n_iterations=1,usePredict_poba=True, scoreFunction=\"amsasimov\", colNames=list(data.columns.values))\n",
        "    #PI_xgb.dislayResults()\n",
        "    plott_xgb = PI_xgb.plotBars()\n",
        "\n",
        "    #LightGBM    \n",
        "    if useLGB:\n",
        "      PI_gbm = PermulationImportance(model=gbm, X=X_test.values,y=y_test,weights=weights_test,\\\n",
        "                             n_iterations=1,usePredict_poba=True, scoreFunction=\"amsasimov\", colNames=list(data.columns.values))\n",
        "      #PI_gbm.dislayResults()\n",
        "      plott_gbm = PI_gbm.plotBars()\n",
        "\n",
        "    # also works with Keras NN\n",
        "    PI_model = PermulationImportance(model=model, X=X_test.values,y=y_test,weights=weights_test,\\\n",
        "                           n_iterations=1,usePredict_poba=False, scoreFunction=\"amsasimov\", colNames=list(data.columns.values))\n",
        "    #PI_model.dislayResults()\n",
        "    plott_model = PI_model.plotBars()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEoxOIiCuvqM"
      },
      "source": [
        "### Hyperparameter optimisation\n",
        "Can be done by hand, with [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
        "\n",
        "Also dedicated packages doing Gaussian process optimisation or 'tree of Parzen estimators' (TPE) (e.g. [hyperopt](https://github.com/hyperopt/hyperop) or [optuna](https://optuna.org/))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vSeMklV8uvqM"
      },
      "source": [
        "import scipy.stats as stats\n",
        "if False:\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "    # specify parameters and distributions to sample from\n",
        "    param_dist_XGB = {'n_estimators': stats.randint(50, 500), #default 100\n",
        "                      'learning_rate': stats.uniform(0.1, 0.5)} #def 0.3 \n",
        "                      #'max_depth': stats.randint(3, 12)} # default 6\n",
        "\n",
        "\n",
        "    # default CV is 5 fold, reduce to 2 for speed concern\n",
        "    # default n_iter is 10 sets of parameters, reduce to 5 for speed concern\n",
        "    gsearch = RandomizedSearchCV(estimator = XGBClassifier(tree_method=\"hist\",use_label_encoder=False,eval_metric='logloss'), \n",
        "                        param_distributions = param_dist_XGB, \n",
        "                        scoring='roc_auc',n_iter=5,cv=2,verbose=2)\n",
        "    gsearch.fit(X_train,y_train, sample_weight=weights_train)\n",
        "\n",
        "    print (\"Best parameters: \",gsearch.best_params_)\n",
        "    print (\"Best score (on train dataset CV): \",gsearch.best_score_)\n",
        "\n",
        "\n",
        "    y_pred_gs = gsearch.predict_proba(X_test)[:,1]\n",
        "    print(\"... corresponding score on test dataset: \",roc_auc_score(y_true=y_test, y_score=y_pred_gs, sample_weight=weights_test))\n",
        "    dfsearch=pd.DataFrame.from_dict(gsearch.cv_results_)\n",
        "    display(dfsearch)\n",
        "    dfsearch.plot.scatter(\"param_n_estimators\",\"mean_test_score\")\n",
        "    #dfsearch.plot.scatter(\"param_max_depth\",\"mean_test_score\")\n",
        "    dfsearch.plot.scatter(\"param_learning_rate\",\"mean_test_score\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}