{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanncoadou/MLtutorials/blob/ESIPAP2023/BDT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcL2rlpraCL2"
      },
      "source": [
        "<h1><center>Machine learning hands-on</center></h1>\n",
        "<h2><center>Decision tree algorithms</center></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYV6fbcnXIR2"
      },
      "source": [
        "# Standard imports and useful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HnV_PMdpCI7"
      },
      "outputs": [],
      "source": [
        "# scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification, make_circles\n",
        "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score, roc_auc_score, roc_curve, RocCurveDisplay\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns # seaborn for nice plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "np.random.seed(31415) # set the np random seed for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATAV2LehCsHL"
      },
      "source": [
        "### Function to plot decision contours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBSIEt7OX2R8"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
        "\n",
        "def my_plot_decision_regions(model, X, y, alpha=1.0, size=25, npts=10000, zoom=0.25, event5=False):\n",
        "  x1min = X[:,0].min() - zoom\n",
        "  x1max = X[:,0].max() + zoom\n",
        "\n",
        "  x2min = X[:,1].min() - zoom\n",
        "  x2max = X[:,1].max() + zoom\n",
        "  \n",
        "  x1 = np.random.uniform(x1min, x1max, npts)\n",
        "  x2 = np.random.uniform(x2min, x2max, npts)\n",
        "\n",
        "  if hasattr(model, \"predict_proba\"):\n",
        "    z = model.predict_proba(np.vstack((x1,x2)).T)\n",
        "  else:\n",
        "    z = model.predict(np.vstack((x1,x2)).T)\n",
        "  \n",
        "  if len(z.shape) == 2:\n",
        "    if z.shape[1] == 1:\n",
        "      z = z.reshape(-1)\n",
        "    elif z.shape[1] == 2:\n",
        "      z = z[:,1].reshape(-1)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  bottom = cm.get_cmap('Oranges', 128)\n",
        "  top = cm.get_cmap('Blues_r', 128)\n",
        "\n",
        "  newcolors = np.vstack((top(np.linspace(0, 1, 128+128)[-128:]),\n",
        "                        bottom(np.linspace(0, 1, 128+128)[:128])))\n",
        "  newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
        "\n",
        "\n",
        "  ax.tricontour(x1, x2, z, levels=np.linspace(0.0-np.finfo(float).eps,1.0+np.finfo(float).eps,20,True), linewidths=0.1, colors='k', antialiased=True)\n",
        "  cntr = ax.tricontourf(x1, x2, z, levels=np.linspace(0.0-np.finfo(float).eps,1.0+np.finfo(float).eps,20,True), cmap=newcmp)\n",
        "  sctr0 = ax.scatter(X[y==0][:,0], X[y==0][:,1], alpha=alpha, s=size, c=\"#1f77b4\", marker=\"s\", edgecolors=\"k\", linewidths=0.5)\n",
        "  sctr1 = ax.scatter(X[y==1][:,0], X[y==1][:,1], alpha=alpha, s=size, c=\"#ff7f0e\",  marker=\"^\", edgecolors=\"k\", linewidths=0.5)\n",
        "  if event5: # showing particular swinger event\n",
        "    sctr2 = ax.scatter(X[4][0], X[4][1], alpha=1, s=size*10, c=\"lightgreen\",  marker=\"X\", edgecolors=\"k\", linewidths=1)\n",
        "  fig.colorbar(cntr, ax=ax)\n",
        "  # ax.set(xlim=(x1min, x1max), ylim=(x2min, x2max))\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQWreSYyC63_"
      },
      "source": [
        "### Function to plot ROC curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LVVBZ4NS6Qp"
      },
      "outputs": [],
      "source": [
        "def my_roc_curves(models, X_test, y_test, weights_test=np.array([])):\n",
        "  if weights_test.size == 0:\n",
        "    weights_test = np.ones(len(X_test))\n",
        "  for i, clf in enumerate(models):\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "      y_preds_clf = clf.predict_proba(X_test)[:,1].reshape(-1)\n",
        "    else:\n",
        "      y_preds_clf = clf.predict(X_test)\n",
        "    fpr_clf,tpr_clf,_ = roc_curve(y_true=y_test, y_score=y_preds_clf, sample_weight=weights_test)\n",
        "    auc_test_clf = roc_auc_score(y_true=y_test, y_score=y_preds_clf, sample_weight=weights_test)\n",
        "    plt.plot(fpr_clf, tpr_clf, label='{} (AUC  = {})'.format(clf.__class__.__name__,np.round(auc_test_clf,decimals=2)))\n",
        "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
        "  plt.xlim([-0.05, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "  plt.legend(loc=\"lower right\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkZHLdFdEDSO"
      },
      "source": [
        "# Defining datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTL8w9wsxHHJ"
      },
      "outputs": [],
      "source": [
        "# X = (x,y) coordinates; y = class\n",
        "X1, y1 = make_circles(n_samples=1000, noise=0.1, factor=0.8)\n",
        "X2, y2 = make_circles(n_samples=1000, noise=0.2, factor=0.2)\n",
        "X = np.vstack((X1,X2/2))\n",
        "y = np.hstack((y1,y2))\n",
        "\n",
        "# Splitting in train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-EYTqhCz2VU"
      },
      "source": [
        "# Classifier zoo\n",
        "\n",
        "Play with various tree-based algorithms as implemented in scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7aFYk4j6y6s"
      },
      "outputs": [],
      "source": [
        "# Add classifiers into a list to access them later\n",
        "classifiers = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xKG-csE30o1"
      },
      "source": [
        "## Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENciMbPgs1of"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZVxFAovs-aB"
      },
      "outputs": [],
      "source": [
        "dtc = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1TbTgmHpgO8"
      },
      "outputs": [],
      "source": [
        "display(dtc.get_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I61-2OGO9tXD"
      },
      "outputs": [],
      "source": [
        "dtc.fit(X_train, y_train);\n",
        "classifiers.append(dtc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm6--R3vIsHx"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(15,10))\n",
        "plot_tree(dtc)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS3I8tf6rkIQ"
      },
      "source": [
        "---\n",
        "How often is the prediction of the decision tree correct? Measured **on the testing or validation sample**, for instance with *accuracy*.\n",
        "\n",
        "*Note*: MANY other measures of performance, see e.g. what is available in [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWtnH5mfHoAc"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy:\",accuracy_score(y_test, dtc.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws6PLQDMXomi"
      },
      "source": [
        "---\n",
        "Access to results:\n",
        "- `predict` returns the class (0 or 1 if binary classifier)\n",
        "- `predict_proba` returns the probability of each class (if available)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMELaRV8WykV"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",dtc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",dtc.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ebgN5Ciu_MU"
      },
      "outputs": [],
      "source": [
        "# Plotting decision contours\n",
        "try:\n",
        "  from mlxtend.plotting import plot_decision_regions\n",
        "except ImportError as e:\n",
        "  !pip install mlxtend\n",
        "  from mlxtend.plotting import plot_decision_regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xQIXsMmvHqT"
      },
      "outputs": [],
      "source": [
        "# practical but limited contour-plotting function\n",
        "plot_decision_regions(X_test, y_test, dtc);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_iWuFOEYTL5"
      },
      "outputs": [],
      "source": [
        "# defined at top of notebook\n",
        "# can use class (0 or 1) or class probability when available\n",
        "my_plot_decision_regions(dtc, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KnNIVndumhX"
      },
      "source": [
        "---\n",
        "Receiver operating characteristic curve (ROC curve) and area under the curve (AUC).\n",
        "\n",
        "<center> <img style=\"display: block; margin-left: auto; margin-right: auto; width: 30%;\" alt=\"ROCcurve\" width=\"30%\" src=\"https://raw.githubusercontent.com/yanncoadou/MLtutorials/main/ROCcurve.png\" > </center>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWUVu3GmISTi"
      },
      "outputs": [],
      "source": [
        "my_roc_curves([dtc], X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7GDJuCPuzp"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBsjZC-1Puzt"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjkcRs55Puzx"
      },
      "outputs": [],
      "source": [
        "#abc = AdaBoostClassifier()\n",
        "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),n_estimators=100)\n",
        "display(abc.get_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "759JYRk4-eRa"
      },
      "outputs": [],
      "source": [
        "abc.fit(X_train, y_train);\n",
        "classifiers.append(abc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ANOY5aZYMK7"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",abc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",abc.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El7EfSiKPuz6"
      },
      "outputs": [],
      "source": [
        "my_plot_decision_regions(abc, X_test, y_test)\n",
        "my_roc_curves([abc], X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQOs0B2ahQZc"
      },
      "source": [
        "## Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA3nD4rov9F5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5RhTx8fv_jj"
      },
      "outputs": [],
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators=400,verbose=1)\n",
        "display(gbc.get_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUSc5Vyf_2Dw"
      },
      "outputs": [],
      "source": [
        "gbc.fit(X_train, y_train)\n",
        "classifiers.append(gbc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_92fwlWPYxyk"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",gbc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",gbc.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e27dSIbwwkUT"
      },
      "outputs": [],
      "source": [
        "my_plot_decision_regions(gbc, X_test, y_test, event5=True)\n",
        "my_roc_curves([gbc], X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8k241SKLPiV"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaIRJLaTLPiZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvj_qdQILPib"
      },
      "outputs": [],
      "source": [
        "rfc = RandomForestClassifier(n_estimators=400,verbose=1)\n",
        "display(rfc.get_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNBNo7-VBKeu"
      },
      "outputs": [],
      "source": [
        "rfc.fit(X_train, y_train)\n",
        "classifiers.append(rfc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtq-96fBe4c7"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",rfc.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",rfc.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9WlAQvDLPie"
      },
      "outputs": [],
      "source": [
        "my_plot_decision_regions(rfc, X_test, y_test)\n",
        "my_roc_curves([rfc], X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p6edHiVc4Id"
      },
      "source": [
        "## Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4mxcyxiCABw"
      },
      "outputs": [],
      "source": [
        "my_roc_curves(classifiers, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STHtukkcHSSN"
      },
      "source": [
        "## Other classifiers\n",
        "We have only seen tree-based classifiers from scikit-learn above. Tree-based algorithms tend to be fast and to work decently out-of-the-box.\n",
        "\n",
        "There are many more types of classifiers:\n",
        "\n",
        "1.   implemented in scikit-learn: see the [user's guide](https://scikit-learn.org/stable/user_guide.html) for A LOT of different algorithms\n",
        "2.   in various other packages:\n",
        "- for decision trees (more powerful than scikit-learn implementations): [XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/), [CatBoost](https://catboost.ai/) (see below)\n",
        "\n",
        "- for neural networks: [TensorFlow](https://www.tensorflow.org/) (tomorrow's tutorial), [PyTorch](https://pytorch.org/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnOE-0N_fdIx"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgqAzaX_W_uK"
      },
      "outputs": [],
      "source": [
        "# preinstalled version 0.9.0 20230127\n",
        "!pip install xgboost --upgrade # install 1.7.3 20230127"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEuLeLx8LLrG"
      },
      "outputs": [],
      "source": [
        "# keep only previous random forest training for comparison\n",
        "classifiers = [rfc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM4NNbJoPZN_"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "# tree_method=\"hist\" is 10 times faster, however less robust against awkwards features\n",
        "#   (not a bad idea to double check without it)\n",
        "# Can even try tree_method=\"gpu_hist\" if proper GPU installation\n",
        "xgb = XGBClassifier(tree_method=\"hist\")\n",
        "\n",
        "xgb.fit(X_train, y_train) # note that XGB 1.3.X requires positive weights\n",
        "classifiers.append(xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvpqMZ1-TpUd"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",xgb.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",xgb.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PaCYWUYK7tr"
      },
      "outputs": [],
      "source": [
        "my_roc_curves(classifiers, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVR4ywDEim1Q"
      },
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J8XlCk0X6xj"
      },
      "outputs": [],
      "source": [
        "# preinstalled version 2.2.3 20230127\n",
        "!pip install lightgbm --upgrade # install 3.3.5 20230127\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDbUCYMOUbPA"
      },
      "outputs": [],
      "source": [
        "gbm = lgb.LGBMClassifier()\n",
        "gbm.fit(X_train, y_train);\n",
        "classifiers.append(gbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAaxHkU6Uwv3"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",gbm.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",gbm.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMs7tT1BNZMO"
      },
      "outputs": [],
      "source": [
        "my_roc_curves(classifiers, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvLySceMI4cT"
      },
      "source": [
        "### CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1PUtBnUI4cZ"
      },
      "outputs": [],
      "source": [
        "# not preinstalled 20230127\n",
        "!pip install catboost # install 1.1.1 20230127\n",
        "import catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCORGF2tVmRF"
      },
      "outputs": [],
      "source": [
        "cat = catboost.CatBoostClassifier()\n",
        "cat.fit(X_train, y_train)\n",
        "classifiers.append(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFCV1iAaVmRL"
      },
      "outputs": [],
      "source": [
        "print(\"Truth:   \\n\",y_test[:5])\n",
        "print(\"predict: \\n\",cat.predict(X_test[:5]))\n",
        "print(\"predict_proba: \\n\",cat.predict_proba(X_test[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoYmGQUuNk9y"
      },
      "outputs": [],
      "source": [
        "my_roc_curves(classifiers, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vsBl3cz2WA"
      },
      "source": [
        "# High energy physics application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0TmE1VLlxyG"
      },
      "source": [
        "## Input dataset\n",
        "\n",
        "Data created from ATLAS Open Data by David Rousseau. See doc:\n",
        "\n",
        "http://opendata.atlas.cern/release/2020/documentation/datasets/intro.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iA5CK6DmhPi"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGmdM_7eReqK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  COLAB = True # if running in COLAB\n",
        "  print(\"You are running on Colab.\")\n",
        "except:\n",
        "  COLAB = False # if not running on COLAB\n",
        "  print(\"You are NOT running on Colab, you need to fix the data file path below.\")\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM7EXXkPRz7P"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "  #### Reading files from Google Drive\n",
        "  # Need a Google account to be identified\n",
        "  #!pip install PyDrive\n",
        "  import os\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROMhtcGHZLbc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "datapath=\"\"\n",
        "if not os.path.isfile(\"dataWW_d1_600k.csv.gz\"):\n",
        "  if COLAB:\n",
        "    #attach dataset from google drive \n",
        "    download = drive.CreateFile({'id': '1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4'})\n",
        "    download.GetContentFile(\"dataWW_d1_600k.csv.gz\")\n",
        "    !ls -lrt\n",
        "  else :\n",
        "    # Make sure the file is available locally. \n",
        "    # Should be downloaded from https://drive.google.com/uc?id=1nlXp7P-xq_jip4aPE0j0mnPhYnIOcBv4\n",
        "    !ls -lrt # what is in the local directory\n",
        "    datapath=\"/directory/where/you/stored/dataWW_d1_600k.csv.gz\"\n",
        "    !ls -lrt {datapath} # what is in the data directory\n",
        "    datapath=os.path.abspath(datapath).replace(\"\\ \", \" \")  # try to normalise the path (annoyance with the space)\n",
        "    print (\"Will take data from : \",datapath)\n",
        "filename=os.path.join(datapath,\"dataWW_d1_600k.csv.gz\")\n",
        "print(\"File to be loaded: \")\n",
        "!ls -l {filename}\n",
        "\n",
        "# Loading dataset\n",
        "dfall = pd.read_csv(filename) \n",
        "print (\"\\nFile loaded with \",dfall.shape[0], \" events \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pum_u34Dlib_"
      },
      "source": [
        "\n",
        "You should now see:\n",
        "\n",
        "`File loaded with  600000  events`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms_kAr_nmWL3"
      },
      "source": [
        "### Checking the content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtKOBCbam1pD"
      },
      "outputs": [],
      "source": [
        "#dumping list of features\n",
        "dfall.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC3MydYcnBzE"
      },
      "outputs": [],
      "source": [
        "#examining first few events\n",
        "display(dfall.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb-Pv4TXnNPQ"
      },
      "outputs": [],
      "source": [
        "#examining feature distributions\n",
        "dfall.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Z7EYWMnyxk"
      },
      "source": [
        "***Event weights***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0VbTZ3dnsNB"
      },
      "outputs": [],
      "source": [
        "label_nevents = (dfall[dfall.label==0].shape[0], dfall[dfall.label==1].shape[0] )\n",
        "print(\"Number of events per class (B, S):\",label_nevents)\n",
        "\n",
        "label_weights = (dfall[dfall.label==0].mcWeight.sum(), dfall[dfall.label==1].mcWeight.sum() ) \n",
        "print(\"Total weight per class (B, S):    \",label_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38t5fWDRf967"
      },
      "source": [
        "## Event selection\n",
        "\n",
        "Only keep events with exactly two leptons for this exercise.\n",
        "\n",
        "Only keep events with positive weight, as many ML tools choke on negative weight.\n",
        "\n",
        "*Note: This is in principle WRONG, only possibly valid if your positive and negative weight events are statistically similar (could then also take the absolute value of the weight to increase statistics).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2wvNoDqgDPq"
      },
      "outputs": [],
      "source": [
        "print (\"Df shape before selection:\", dfall.shape)\n",
        "\n",
        "fulldata=dfall[ (dfall.lep_n==2) & (dfall.mcWeight > 0)]  \n",
        "\n",
        "print (\"Df shape after selection: \",fulldata.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM01cgNliKeV"
      },
      "outputs": [],
      "source": [
        "# Hide label and weights in separate vectors (not discriminating features)\n",
        "# WARNING : there should be neither selection nor shuffling later on! (otherwise misalignement)\n",
        "target = fulldata[\"label\"]\n",
        "weights = fulldata[\"mcWeight\"]\n",
        "\n",
        "# for simplicity only keep some features\n",
        "# this is actually making a deep copy from fulldata\n",
        "data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_phi_0', 'lep_phi_1'])\n",
        "#data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1', 'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
        "#       'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1']\n",
        "\n",
        "print (\"Df shape of dataset to be used:\",data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEzVz2fUvUNs"
      },
      "source": [
        "### Plotting variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16cH0csMjoLZ"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "data['met_et'].plot.hist(title='Missing Transverse Energy', log=True, ax=ax[0])\n",
        "data[data.lep_pt_0+data.lep_pt_1>1000]['met_et'].plot.hist(bins=np.linspace(0,400,50),title='Missing Transverse Energy for large lepton Pt', ax=ax[1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqwSM579lxQI"
      },
      "outputs": [],
      "source": [
        "ax=data[target==0].plot.scatter(x='met_et', y='lep_pt_0',color=\"b\",label=\"B\")\n",
        "data[target==1].plot.scatter(x='met_et', y='lep_pt_0',color=\"r\",label=\"S\",alpha=.5,ax=ax);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEKfvFJcuMdt"
      },
      "outputs": [],
      "source": [
        "data[data.lep_pt_0+data.lep_pt_1>2000].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFsOPQfvpK7"
      },
      "outputs": [],
      "source": [
        "ax=data[target==0].hist(weights=weights[target==0],figsize=(15,12),bins=50,color='b',alpha=0.5,density=True,label=\"B\")\n",
        "ax=ax.flatten()[:data.shape[1]] # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
        "data[target==1].hist(weights=weights[target==1],figsize=(15,12),bins=50,color='r',alpha=0.5,density=True,ax=ax,label=\"S\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iekW3mH_2bv4"
      },
      "source": [
        "## Sample splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd_EetqH2XRo"
      },
      "outputs": [],
      "source": [
        "np.random.seed(31415) # set the random seed (used for the train/test splitting)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_size = 0.75 # fraction of sample used for training\n",
        "val_size = 0.2 # fraction of training sample used for validation\n",
        "\n",
        "# split only train/test\n",
        "#X_train, X_test, y_train, y_test, weights_train, weights_test = \\\n",
        "#    train_test_split(data, target, weights, train_size=train_size)\n",
        "\n",
        "#split in train/validation/test\n",
        "X_holdout, X_test, y_holdout, y_test, weights_holdout, weights_test = \\\n",
        "    train_test_split(data, target, weights, train_size=train_size)\n",
        "X_train, X_val, y_train, y_val, weights_train, weights_val = \\\n",
        "    train_test_split(X_holdout, y_holdout, weights_holdout, train_size=1-val_size)\n",
        "\n",
        "print(\"Training sample:  \", X_train.shape)\n",
        "print(\"Validation sample:\", X_val.shape)\n",
        "print(\"Testing sample:   \", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV_N1xxRYmJr"
      },
      "outputs": [],
      "source": [
        "# Weight handling\n",
        "class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
        "print (\"class_weights_train (B, S):\",class_weights_train)\n",
        "\n",
        "for i in range(len(class_weights_train)): # could have more than two classes\n",
        "    weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
        "    weights_test[y_test == i] *= 1/(1-train_size) # increase test weight to compensate for sampling\n",
        "    weights_val[y_val == i] *= 1/val_size/train_size # increase val weight to compensate for samplings\n",
        "    \n",
        "print (\"Test:  total weight sig\", weights_test[y_test == 1].sum())\n",
        "print (\"Test:  total weight bkg\", weights_test[y_test == 0].sum())\n",
        "print (\"Train: total weight sig\", weights_train[y_train == 1].sum())\n",
        "print (\"Train: total weight bkg\", weights_train[y_train == 0].sum())\n",
        "print (\"Val:   total weight sig\", weights_val[y_val == 1].sum())\n",
        "print (\"Val:   total weight bkg\", weights_val[y_val == 0].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZoHK3oOnnnM"
      },
      "source": [
        "## Boosted tree training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHdx9nKp3i2l"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
        "xgb = XGBClassifier(tree_method=\"hist\")\n",
        "xgb.fit(X_train, y_train.values,\n",
        "        sample_weight=weights_train.values) # note that XGB 1.3.X requires positive weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH7P5yM54dlW"
      },
      "outputs": [],
      "source": [
        "my_roc_curves([xgb], X_test, y_test, weights_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwsPP1QZ4llA"
      },
      "source": [
        "### Training monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGtNGfax8Lvj"
      },
      "outputs": [],
      "source": [
        "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
        "weight_set = [weights_train, weights_val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8iTUmU4ydUw"
      },
      "outputs": [],
      "source": [
        "# previous XGBoost instantiation:\n",
        "#   xgb = XGBClassifier(tree_method=\"hist\")\n",
        "xgb = XGBClassifier(tree_method=\"hist\",\n",
        "                    eval_metric=[\"logloss\",\"auc\",\"error\"])\n",
        "# Note: could also have used set_params instead of passing eval_metric in constructor:\n",
        "#       xgb = XGBClassifier(tree_method=\"hist\")\n",
        "#       xgb.set_params(eval_metric=[\"logloss\",\"auc\",\"error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ep4n42osnl8"
      },
      "outputs": [],
      "source": [
        "# previous training command line:\n",
        "#  xgb.fit(X_train, y_train,\n",
        "#          sample_weight=weights_train.values)\n",
        "xgb.fit(X_train, y_train,\n",
        "        sample_weight=weights_train.values,\n",
        "        eval_set=eval_set,\n",
        "        sample_weight_eval_set=weight_set);\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdGYPNAsknR_"
      },
      "outputs": [],
      "source": [
        "# retrieve performance metrics\n",
        "results = xgb.evals_result()\n",
        "epochs = len(results['validation_0']['error'])\n",
        "x_axis = range(0, epochs)\n",
        "# plot log loss\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['logloss'], label='Validation')\n",
        "ax.legend()\n",
        "plt.ylabel('Log Loss')\n",
        "plt.title('XGBoost Log Loss')\n",
        "plt.show()\n",
        "# plot classification error\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['error'], label='Validation')\n",
        "ax.legend()\n",
        "plt.ylabel('Classification Error')\n",
        "plt.title('XGBoost Classification Error')\n",
        "plt.show()\n",
        "# plot AUC\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
        "ax.plot(x_axis, results['validation_1']['auc'], label='Validation')\n",
        "ax.legend()\n",
        "plt.ylabel('AUC')\n",
        "plt.title('XGBoost Area under the curve (AUC)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk2d4D4B7v-V"
      },
      "outputs": [],
      "source": [
        "## Adding early stopping condition (will use last entry in eval_metric to monitor early stopping\n",
        "xgb.set_params(early_stopping_rounds=10)\n",
        "xgb.fit(X_train, y_train,\n",
        "        sample_weight=weights_train.values,\n",
        "        eval_set=eval_set,\n",
        "        sample_weight_eval_set=weight_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSjyLfr3s69D"
      },
      "source": [
        "*Why did it stop so early, after 11 trees only?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8eZxxNQ-y3i"
      },
      "source": [
        "*Play with* `n_estimators` *and stopping conditions to find better classifier.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFitqUnjssKx"
      },
      "outputs": [],
      "source": [
        "#xgb = XGBClassifier(tree_method=\"hist\")\n",
        "#xgb.fit(\"FIX ME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R606WhZ9lGt"
      },
      "source": [
        "###Learning curve\n",
        "Compute the AUC by varying the number of training events. Validation set remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FGlKG6yjy-"
      },
      "outputs": [],
      "source": [
        "train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1]\n",
        "ntrains=[]\n",
        "val_aucs=[]\n",
        "train_aucs=[]\n",
        "times=[]\n",
        "\n",
        "import time\n",
        "\n",
        "xgb = XGBClassifier(tree_method=\"hist\")\n",
        "\n",
        "for train_size in train_sizes:\n",
        "  ntrain=int(len(X_train)*train_size)\n",
        "  print(\"Training with \",ntrain,\" events\")\n",
        "  ntrains+=[ntrain]\n",
        "  starting_time = time.time()\n",
        "\n",
        "  # train using the first ntrain event of the training dataset\n",
        "  xgb.fit(X_train[:ntrain], y_train[:ntrain],sample_weight=weights_train[:ntrain].values)\n",
        "  training_time = time.time( ) - starting_time\n",
        "  times+=[training_time]\n",
        "\n",
        "  # score on validation dataset (always the same)\n",
        "  y_val_xgb=xgb.predict_proba(X_val)[:,1]\n",
        "  auc_val_xgb = roc_auc_score(y_true=y_val, y_score=y_val_xgb)\n",
        "  val_aucs+=[auc_val_xgb]\n",
        "\n",
        "  # score on the train dataset \n",
        "  y_train_xgb=xgb.predict_proba(X_train[:ntrain])[:,1]\n",
        "  auc_train_xgb = roc_auc_score(y_true=y_train[:ntrain], y_score=y_train_xgb)\n",
        "  train_aucs+=[auc_train_xgb]\n",
        "\n",
        "dflearning=pd.DataFrame({\"Ntraining\":ntrains,\n",
        "                         \"val_auc\":val_aucs,\n",
        "                         \"train_auc\":train_aucs,\n",
        "                         \"time\":times})\n",
        "display(dflearning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-G75wEil_q-"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(12, 5))\n",
        "ax[0].grid()\n",
        "ax[0].plot('Ntraining','train_auc',\"o-\",data=dflearning,label=\"Train\",color=\"r\")\n",
        "ax[0].plot(dflearning.Ntraining,dflearning.val_auc,\"o-\",label=\"Validation\",color=\"b\")\n",
        "ax[0].set_xlabel(\"Training examples\")\n",
        "ax[0].set_ylabel(\"AUC\")\n",
        "ax[0].legend()\n",
        "ax[1].grid()\n",
        "ax[1].plot('Ntraining','time',\"o-\",data=dflearning)\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Training examples\")\n",
        "ax[1].set_ylabel(\"Fit time [s]\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzCKxGXHjgTh"
      },
      "source": [
        "Could also use `learning_curve` in sklearn.\n",
        "\n",
        "*Notes*:\n",
        "* it does not handle event weights\n",
        "* it does not allow to control testing dataset size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qygIGQDhfXNd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "train_sizes,train_scores,test_scores,fit_times,_ = learning_curve(\n",
        "     XGBClassifier(tree_method=\"hist\"),\n",
        "     X_train,y_train,\n",
        "     train_sizes=[0.01,0.05,0.1,0.2,0.5,0.75,1],                  \n",
        "     scoring='roc_auc',cv=5,\n",
        "     return_times=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFyLSJctfg-s"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(12, 5))\n",
        "ax[0].set_title('Learning curves')\n",
        "ax[0].set_xlabel(\"Training examples\")\n",
        "ax[0].set_ylabel(\"AUC\")\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "fit_times_mean = np.mean(fit_times, axis=1)\n",
        "fit_times_std = np.std(fit_times, axis=1)\n",
        "ax[0].grid()\n",
        "ax[0].fill_between(\n",
        "        train_sizes,\n",
        "        train_scores_mean - train_scores_std,\n",
        "        train_scores_mean + train_scores_std,\n",
        "        alpha=0.3,\n",
        "        color=\"r\",\n",
        ")\n",
        "ax[0].fill_between(\n",
        "        train_sizes,\n",
        "        test_scores_mean - test_scores_std,\n",
        "        test_scores_mean + test_scores_std,\n",
        "        alpha=0.3,\n",
        "        color=\"b\",\n",
        ")\n",
        "ax[0].plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Train\")\n",
        "ax[0].plot(train_sizes, test_scores_mean, \"o-\", color=\"b\", label=\"Validation\")\n",
        "ax[0].legend(loc=\"best\");\n",
        "\n",
        "# Plot fit time vs Ntraining\n",
        "ax[1].grid()\n",
        "ax[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
        "ax[1].fill_between(\n",
        "        train_sizes,\n",
        "        fit_times_mean - fit_times_std,\n",
        "        fit_times_mean + fit_times_std,\n",
        "        alpha=0.3,\n",
        ")\n",
        "ax[1].set_xlabel(\"Training examples\")\n",
        "ax[1].set_ylabel(\"Fit time [s]\")\n",
        "ax[1].set_title(\"Scalability of model\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoN_bqOHAkN2"
      },
      "source": [
        "### Model saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIl07fGCAkN6"
      },
      "outputs": [],
      "source": [
        "xgb.save_model(\"XGBoost.model\")\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRXtTz74H1t"
      },
      "source": [
        "Reload a trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlFx_VlSAkOA"
      },
      "outputs": [],
      "source": [
        "print(\"Prediction from original model:\")\n",
        "display(xgb.predict_proba(X_test[:5]))\n",
        "\n",
        "reloaded_model=XGBClassifier()\n",
        "reloaded_model.load_model(\"XGBoost.model\")\n",
        "print(\"Prediction from reloaded model:\")\n",
        "display(reloaded_model.predict_proba(X_test[:5]))\n",
        "\n",
        "try:\n",
        "  np.testing.assert_allclose(\n",
        "      xgb.predict_proba(X_test), reloaded_model.predict_proba(X_test)\n",
        "  )\n",
        "  print(\"Original and reloaded models are identical\")\n",
        "except AssertionError:\n",
        "  print(\"Watch out: original and reloaded models are different\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwWsfyeZzjS9"
      },
      "source": [
        "## Physics performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YniuFUBNMeQ"
      },
      "source": [
        "### Feature importance\n",
        "Feature importance allows to display the importance of each feature without rerunnning the training. It is obtained from internal algorithm quantities, like cumulated decrease of impurity, *during training*. Magnitude is arbitrary. It can be used as a not very reliable indication of which features are the most discriminant *for this particular training*.\n",
        "\n",
        "Very straightforward with decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdbAE88HCWBn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "classifiers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O-fUIiJM7w4"
      },
      "outputs": [],
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators=10,verbose=1)\n",
        "gbc.fit(X_train, y_train, sample_weight=weights_train)\n",
        "classifiers.append(gbc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHVWv_f5NMeR"
      },
      "outputs": [],
      "source": [
        "plt.bar(data.columns.values, gbc.feature_importances_)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Feature importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXVHA_8g4gx"
      },
      "source": [
        "*What about a different tree classifier?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbkTl8I8PyMi"
      },
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(tree_method=\"hist\")\n",
        "xgb.fit(X_train, y_train.values, sample_weight=weights_train.values)\n",
        "classifiers.append(xgb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z04B3jCjjSZj"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "gbm = lgb.LGBMClassifier()\n",
        "gbm.fit(X_train, y_train.values,sample_weight=weights_train.values)\n",
        "classifiers.append(gbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg2PoX9w76Dg"
      },
      "outputs": [],
      "source": [
        "import catboost\n",
        "cat = catboost.CatBoostClassifier()\n",
        "cat.fit(X_train, y_train,sample_weight=weights_train.values, verbose=False);\n",
        "classifiers.append(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HCQrCJdLO0S"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(2, 2, figsize=(18, 10))\n",
        "for i, clf in enumerate(classifiers, start=0):\n",
        "  ax[i//2,i%2].bar(data.columns.values, clf.feature_importances_)\n",
        "  #ax[i//2,i%2].tick_params(labelrotation=45)\n",
        "  ax[i//2,i%2].set_title(\"{} feature importance\".format(clf.__class__.__name__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK_cRgF5uOt_"
      },
      "source": [
        "*What conclusions can you draw from these plots?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xPybEujNMeS"
      },
      "source": [
        "### Permutation importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiPAfpWquvqP"
      },
      "source": [
        "A better way to show the importance of each feature is Permutation Importance, where each feature in turn is replaced by an instance of an other event (effectively switching it off by randomising).\n",
        "\n",
        "Works on any classifier, not just DT-based. Can be estimated on any sample, not just training set.\n",
        "\n",
        "However, report can be misleading in case of highly correlated variables.\n",
        "\n",
        "Available in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html).\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XDKP8b5PAfi"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "importances = []\n",
        "for clf in classifiers:\n",
        "  result = permutation_importance(clf, X_test, y_test, n_repeats=1, random_state=42, n_jobs=2)\n",
        "  importances.append(pd.Series(result.importances_mean, index=list(data.columns.values)))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmJP1WH5PoOW"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(2, 2, figsize=(18, 10))\n",
        "for i, (clf,importance) in enumerate(zip(classifiers,importances), start=0):\n",
        "  importance.plot.bar(ax = ax[i//2,i%2], subplots=True)\n",
        "  ax[i//2,i%2].tick_params(labelrotation=0)\n",
        "  ax[i//2,i%2].set_title(\"{} feature importance\".format(clf.__class__.__name__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_7NJTDOF0ss"
      },
      "source": [
        "Another implementation targetting HEP:\n",
        "\n",
        "https://github.com/aghoshpub/permutationImportancePhysics \n",
        "\n",
        "In particular it allows to : \n",
        "   * use event weights\n",
        "   * display directly the loss in whatever criterion (ROC auc, asimov significance) when the feature is switched off\n",
        "   * display the feature importance for a specific subset (for example the most signal like)\n",
        "   * it can even display which feature has the largest impact on systematics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZsHARtTuvqQ"
      },
      "outputs": [],
      "source": [
        "!pip install PermutationImportancePhysics\n",
        "from permutationimportancephysics.PermutationImportance import PermulationImportance # note the delibrate typo PermuLation\n",
        "#XGBoost\n",
        "PI_xgb = PermulationImportance(model=xgb, X=X_test.values,y=y_test,weights=weights_test,\\\n",
        "                       n_iterations=1,usePredict_poba=True, scoreFunction=\"amsasimov\", colNames=list(data.columns.values))\n",
        "#PI_xgb.dislayResults()\n",
        "plott_xgb = PI_xgb.plotBars()\n",
        "\n",
        "#LightGBM    \n",
        "PI_gbm = PermulationImportance(model=gbm, X=X_test.values,y=y_test,weights=weights_test,\\\n",
        "                         n_iterations=1,usePredict_poba=True, scoreFunction=\"amsasimov\", colNames=list(data.columns.values))\n",
        "#PI_gbm.dislayResults()\n",
        "plott_gbm = PI_gbm.plotBars()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsMSGl-Kwql"
      },
      "source": [
        "### Significance\n",
        "\n",
        "Asimov significance (from [arXiv:1007.1727](https://arxiv.org/abs/1007.1727) eq. 97):\n",
        "\n",
        "> AMS = $\\sqrt{2\\left((s+b)\\ln\\left(1+\\frac{s}{b}\\right) - s\\right)} = \\frac{s}{\\sqrt{b}}\\left(1+\\mathcal{O}(s/b)\\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qubY3CMNKwql"
      },
      "outputs": [],
      "source": [
        "from math import sqrt, log\n",
        "def amsasimov(s,b):\n",
        "  if b<=0 or s<=0:\n",
        "      return 0\n",
        "  try:\n",
        "      return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
        "  except ValueError:\n",
        "      print(1+float(s)/b)\n",
        "      print (2*((s+b)*log(1+float(s)/b)-s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxd3r5_Q2NRD"
      },
      "outputs": [],
      "source": [
        "vamsasimovs = []\n",
        "for clf in classifiers:\n",
        "  y_pred_clf = clf.predict_proba(X_test)[:,1].reshape(-1)\n",
        "  int_pred_test_sig_clf = [weights_test[(y_test ==1) & (y_pred_clf > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "  int_pred_test_bkg_clf = [weights_test[(y_test ==0) & (y_pred_clf > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
        "\n",
        "  vamsasimov_clf = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig_clf,int_pred_test_bkg_clf)]\n",
        "  print(\"Z({}): {}\".format(clf.__class__.__name__,np.round(max(vamsasimov_clf),decimals=3)))\n",
        "  vamsasimovs.append(vamsasimov_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBN4XTzpdDKw"
      },
      "outputs": [],
      "source": [
        "for i, (clf,vamsasimov_clf) in enumerate(zip(classifiers,vamsasimovs), start=0):\n",
        "  plt.plot(np.linspace(0,1,num=50),vamsasimov_clf, label='AMS (Z_max({}) = {})'.format(clf.__class__.__name__,np.round(max(vamsasimov_clf),decimals=3)))\n",
        "plt.title(\"Significance\")\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Significance\")\n",
        "plt.legend()\n",
        "#plt.savefig(\"Significance.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEoxOIiCuvqM"
      },
      "source": [
        "### Hyperparameter optimisation\n",
        "Can be done by hand, with [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) or [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
        "\n",
        "Also dedicated packages doing Gaussian process optimisation or 'tree of Parzen estimators' (TPE) (e.g. [hyperopt](https://github.com/hyperopt/hyperop) or [optuna](https://optuna.org/))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGs6L9FEg_9A",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist_XGB = {'n_estimators': stats.randint(10, 500), #default 100\n",
        "                  'learning_rate': stats.uniform(0.01, 0.5)} #def 0.3 \n",
        "                  #'max_depth': stats.randint(3, 12)} # default 6\n",
        "\n",
        "# default CV is 5 fold, reduce to 2 for speed concern\n",
        "# default n_iter is 10 sets of parameters, reduce to 5 for speed concern\n",
        "gsearch = RandomizedSearchCV(estimator = XGBClassifier(tree_method=\"hist\"), \n",
        "                             param_distributions = param_dist_XGB, \n",
        "                             scoring='roc_auc',n_iter=5,cv=2,verbose=2)\n",
        "gsearch.fit(X_train,y_train, sample_weight=weights_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdgWnDS6Jqyg"
      },
      "outputs": [],
      "source": [
        "print (\"Best parameters: \",gsearch.best_params_)\n",
        "print (\"Best score (on train dataset CV): \",gsearch.best_score_)\n",
        "# Best model directly accessible if refit=True (default)\n",
        "y_pred_gs = gsearch.predict_proba(X_test)[:,1]\n",
        "print(\"... corresponding score on test dataset: \",roc_auc_score(y_true=y_test, y_score=y_pred_gs))\n",
        "\n",
        "dfsearch=pd.DataFrame.from_dict(gsearch.cv_results_)\n",
        "display(dfsearch.head())\n",
        "\n",
        "fig,ax=plt.subplots(1, 3, figsize=(15, 5))\n",
        "dfsearch.plot(\"param_n_estimators\",\"mean_test_score\",yerr=\"std_test_score\",linestyle = 'None',marker=\"o\", ax=ax[0])\n",
        "ax[0].scatter(gsearch.best_params_['n_estimators'],gsearch.best_score_,color='red',marker=\"*\",s=100,zorder=5)\n",
        "dfsearch.plot(\"param_learning_rate\",\"mean_test_score\",yerr=\"std_test_score\",linestyle = 'None',marker=\"o\", ax=ax[1])\n",
        "ax[1].scatter(gsearch.best_params_['learning_rate'],gsearch.best_score_,color='red',marker=\"*\",s=100,zorder=5)\n",
        "#dfsearch.plot(\"param_max_depth\",\"mean_test_score\",yerr=\"std_test_score\",linestyle = 'None',marker=\"o\", ax=ax[2])\n",
        "#ax[2].scatter(gsearch.best_params_['max_depth'],gsearch.best_score_,color='red',marker=\"*\",s=100,zorder=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmRF8lnLvUnq"
      },
      "source": [
        "# Your turn\n",
        "Try and optimise a classifier on this dataset. You can play with:\n",
        "*   type of classifier\n",
        "*   hyperparameters of classifier\n",
        "*   input variables\n",
        "*   figure of merit\n",
        "\n",
        "Remember that some algorithms are (much) more time consuming than others, and therefore take more time to optimise.\n",
        "\n",
        "You can optimise by hand, or using some of the tools presented above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ibcvG9pwP0b"
      },
      "outputs": [],
      "source": [
        "#your code goes here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AIsMSGl-Kwql",
        "8YniuFUBNMeQ",
        "8xPybEujNMeS",
        "AEoxOIiCuvqM"
      ],
      "name": "BDT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}